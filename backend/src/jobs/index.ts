import cron from 'node-cron';
import Bull from 'bull';
import { Pool } from 'pg';
import { logger } from '../utils/logger';
import { config } from '../config';
import { connectDatabase } from '../database/connection';
import { RouteOptimizer } from '../services/routeOptimizer';
import { PriceScanner } from '../services/priceScanner';
import { AlertService } from '../services/alertService';
import { redis } from '../services/redis';

// Queues Bull pour les jobs asynchrones
const scanQueue = new Bull('price-scanning', config.redis.url);
const alertQueue = new Bull('alert-sending', config.redis.url);
const optimizationQueue = new Bull('route-optimization', config.redis.url);
const cleanupQueue = new Bull('data-cleanup', config.redis.url);

// Services
let routeOptimizer: RouteOptimizer;
let priceScanner: PriceScanner;
let alertServiceInstance: AlertService;

/**
 * Initialiser tous les jobs et workers
 */
export async function initializeJobs(): Promise<void> {
  const db = await connectDatabase.getPool();
  
  // Initialiser les services
  routeOptimizer = new RouteOptimizer(db);
  priceScanner = new PriceScanner(db);
  alertServiceInstance = new AlertService(db);

  // Configurer les workers
  setupWorkers();

  // Configurer les cron jobs
  setupCronJobs();

  // Listener pour les √©v√©nements Redis
  setupEventListeners();

  logger.info('‚úÖ Jobs et workers initialis√©s');
}

/**
 * Configurer les workers Bull
 */
function setupWorkers(): void {
  // Worker de scan de prix
  scanQueue.process('scan-route', 5, async (job) => {
    const { routeId } = job.data;
    logger.info(`üîç Scan de la route ${routeId}`);
    
    try {
      await priceScanner.scanRoute(routeId);
      return { status: 'completed', routeId };
    } catch (error) {
      logger.error(`Erreur scan route ${routeId}:`, error);
      throw error;
    }
  });

  // Worker d'envoi d'alertes
  alertQueue.process('send-anomaly-alerts', 10, async (job) => {
    const { anomalyId } = job.data;
    logger.info(`üìß Envoi des alertes pour l'anomalie ${anomalyId}`);
    
    try {
      await alertServiceInstance.sendAnomalyAlerts(anomalyId);
      return { status: 'completed', anomalyId };
    } catch (error) {
      logger.error(`Erreur envoi alertes ${anomalyId}:`, error);
      throw error;
    }
  });

  // Worker d'optimisation des routes
  optimizationQueue.process('optimize-routes', async (job) => {
    logger.info('üéØ Optimisation des routes en cours...');
    
    try {
      const result = await routeOptimizer.optimizeRouteAllocation();
      return { 
        status: 'completed', 
        reallocations: result.reallocations.length,
        estimatedCalls: result.estimatedMonthlyCalls 
      };
    } catch (error) {
      logger.error('Erreur optimisation routes:', error);
      throw error;
    }
  });

  // Worker de nettoyage
  cleanupQueue.process('cleanup-old-data', async (job) => {
    logger.info('üßπ Nettoyage des donn√©es anciennes...');
    
    try {
      const result = await cleanupOldData();
      return { status: 'completed', ...result };
    } catch (error) {
      logger.error('Erreur nettoyage:', error);
      throw error;
    }
  });

  // Gestion des erreurs et √©v√©nements
  [scanQueue, alertQueue, optimizationQueue, cleanupQueue].forEach(queue => {
    queue.on('completed', (job, result) => {
      logger.info(`‚úÖ Job ${job.name} compl√©t√©:`, result);
    });

    queue.on('failed', (job, err) => {
      logger.error(`‚ùå Job ${job.name} √©chou√©:`, err);
    });

    queue.on('stalled', (job) => {
      logger.warn(`‚ö†Ô∏è Job ${job.name} bloqu√©`);
    });
  });
}

/**
 * Configurer les t√¢ches cron
 */
function setupCronJobs(): void {
  // Scan continu des routes (toutes les 30 minutes)
  cron.schedule('*/30 * * * *', async () => {
    logger.info('‚è∞ D√©marrage du scan p√©riodique des routes');
    
    try {
      // V√©rifier le quota API
      const usage = await routeOptimizer.getMonthlyApiUsage();
      const remaining = config.limits.api.monthlyLimit - usage;
      
      if (remaining < 100) {
        logger.warn('Quota API trop bas, scan annul√©');
        return;
      }

      // R√©cup√©rer les routes √† scanner
      const routes = await routeOptimizer.getNextRoutesToScan(10);
      
      // Ajouter les jobs de scan
      for (const route of routes) {
        await scanQueue.add('scan-route', {
          routeId: route.id
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000
          },
          removeOnComplete: true,
          removeOnFail: false
        });
        
        // Mettre √† jour le statut
        await routeOptimizer.updateRouteAfterScan(route.id, true);
      }
      
      logger.info(`üìã ${routes.length} routes ajout√©es √† la queue de scan`);
    } catch (error) {
      logger.error('Erreur scan p√©riodique:', error);
    }
  });

  // Optimisation des routes (tous les jours √† 3h)
  cron.schedule('0 3 * * *', async () => {
    logger.info('‚è∞ Optimisation quotidienne des routes');
    
    await optimizationQueue.add('optimize-routes', {}, {
      attempts: 1,
      removeOnComplete: true
    });
  });

  // Envoi des digests quotidiens (tous les jours √† 8h)
  cron.schedule('0 8 * * *', async () => {
    logger.info('‚è∞ Envoi des digests quotidiens');
    
    try {
      const db = await connectDatabase.getPool();
      const users = await db.query(`
        SELECT id FROM users 
        WHERE email_verified = true 
          AND (preferences->>'alert_frequency')::text = 'daily'
          AND status != 'free'
      `);
      
      for (const user of users.rows) {
        await alertQueue.add('send-digest', {
          userId: user.id,
          frequency: 'daily'
        }, {
          delay: Math.random() * 3600000, // √âtaler sur 1h
          attempts: 2
        });
      }
      
      logger.info(`üìä ${users.rows.length} digests quotidiens programm√©s`);
    } catch (error) {
      logger.error('Erreur programmation digests:', error);
    }
  });

  // Envoi des digests hebdomadaires (tous les lundis √† 9h)
  cron.schedule('0 9 * * 1', async () => {
    logger.info('‚è∞ Envoi des digests hebdomadaires');
    
    try {
      const db = await connectDatabase.getPool();
      const users = await db.query(`
        SELECT id FROM users 
        WHERE email_verified = true 
          AND (preferences->>'alert_frequency')::text = 'weekly'
      `);
      
      for (const user of users.rows) {
        await alertQueue.add('send-digest', {
          userId: user.id,
          frequency: 'weekly'
        }, {
          delay: Math.random() * 7200000, // √âtaler sur 2h
          attempts: 2
        });
      }
      
      logger.info(`üìä ${users.rows.length} digests hebdomadaires programm√©s`);
    } catch (error) {
      logger.error('Erreur programmation digests hebdo:', error);
    }
  });

  // Nettoyage des donn√©es (tous les jours √† 4h)
  cron.schedule('0 4 * * *', async () => {
    logger.info('‚è∞ Nettoyage quotidien des donn√©es');
    
    await cleanupQueue.add('cleanup-old-data', {}, {
      attempts: 1,
      removeOnComplete: true
    });
  });

  // Calcul des m√©triques quotidiennes (tous les jours √† 1h)
  cron.schedule('0 1 * * *', async () => {
    logger.info('‚è∞ Calcul des m√©triques quotidiennes');
    
    try {
      await calculateDailyMetrics();
    } catch (error) {
      logger.error('Erreur calcul m√©triques:', error);
    }
  });

  // Rafra√Æchissement du cache des m√©triques (toutes les 5 minutes)
  cron.schedule('*/5 * * * *', async () => {
    try {
      const db = await connectDatabase.getPool();
      await db.query('REFRESH MATERIALIZED VIEW CONCURRENTLY dashboard_metrics');
      logger.debug('üìä Vue mat√©rialis√©e rafra√Æchie');
    } catch (error) {
      logger.error('Erreur refresh metrics:', error);
    }
  });

  logger.info('‚úÖ Cron jobs configur√©s');
}

/**
 * Configurer les listeners d'√©v√©nements Redis
 */
function setupEventListeners(): void {
  const subscriber = redis.duplicate();
  
  subscriber.on('message', async (channel, message) => {
    try {
      const data = JSON.parse(message);
      
      switch (channel) {
        case 'anomaly:detected':
          // Nouvelle anomalie d√©tect√©e, envoyer les alertes
          await alertQueue.add('send-anomaly-alerts', {
            anomalyId: data.anomalyId
          }, {
            attempts: 3,
            backoff: {
              type: 'exponential',
              delay: 1000
            }
          });
          break;
          
        case 'price:anomaly':
          // Log pour monitoring
          logger.info(`üéØ Anomalie prix: ${data.route} √† ${data.price}‚Ç¨ (-${data.discount}%)`);
          break;
          
        default:
          logger.debug(`Message re√ßu sur ${channel}:`, data);
      }
    } catch (error) {
      logger.error('Erreur traitement message Redis:', error);
    }
  });

  subscriber.subscribe('anomaly:detected', 'price:anomaly');
  logger.info('‚úÖ Listeners Redis configur√©s');
}

/**
 * Nettoyer les donn√©es anciennes
 */
async function cleanupOldData(): Promise<any> {
  const db = await connectDatabase.getPool();
  const results = {
    priceHistory: 0,
    expiredAnomalies: 0,
    oldAlerts: 0,
    apiCalls: 0
  };

  try {
    // Nettoyer l'historique des prix > 180 jours
    const ph = await db.query(`
      DELETE FROM price_history 
      WHERE created_at < NOW() - INTERVAL '180 days'
    `);
    results.priceHistory = ph.rowCount || 0;

    // Nettoyer les anomalies expir√©es > 30 jours
    const an = await db.query(`
      DELETE FROM anomalies 
      WHERE expires_at < NOW() - INTERVAL '30 days'
    `);
    results.expiredAnomalies = an.rowCount || 0;

    // Nettoyer les alertes > 90 jours
    const al = await db.query(`
      DELETE FROM alerts 
      WHERE created_at < NOW() - INTERVAL '90 days'
    `);
    results.oldAlerts = al.rowCount || 0;

    // Nettoyer les logs API > 30 jours
    const ac = await db.query(`
      DELETE FROM api_calls 
      WHERE created_at < NOW() - INTERVAL '30 days'
    `);
    results.apiCalls = ac.rowCount || 0;

    logger.info('üßπ Nettoyage termin√©:', results);
    return results;
  } catch (error) {
    logger.error('Erreur nettoyage donn√©es:', error);
    throw error;
  }
}

/**
 * Calculer les m√©triques quotidiennes
 */
async function calculateDailyMetrics(): Promise<void> {
  const db = await connectDatabase.getPool();
  const yesterday = new Date();
  yesterday.setDate(yesterday.getDate() - 1);
  
  try {
    await db.query('SELECT calculate_daily_metrics($1)', [yesterday]);
    logger.info(`üìä M√©triques calcul√©es pour ${yesterday.toISOString().split('T')[0]}`);
  } catch (error) {
    logger.error('Erreur calcul m√©triques quotidiennes:', error);
    throw error;
  }
}

/**
 * Arr√™t propre des jobs
 */
export async function shutdownJobs(): Promise<void> {
  logger.info('Arr√™t des jobs...');
  
  // Arr√™ter les cron jobs
  cron.getTasks().forEach((task) => task.stop());
  
  // Fermer les queues Bull
  await Promise.all([
    scanQueue.close(),
    alertQueue.close(),
    optimizationQueue.close(),
    cleanupQueue.close()
  ]);
  
  logger.info('‚úÖ Jobs arr√™t√©s proprement');
}